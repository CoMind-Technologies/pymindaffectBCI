{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import mindaffectBCI.decoder\n",
    "from mindaffectBCI.decoder.UtopiaDataInterface import butterfilt_and_downsample\n",
    "from mindaffectBCI.decoder.devent2stimsequence import devent2stimSequence, upsample_stimseq\n",
    "from mindaffectBCI.decoder.decodingSupervised import decodingSupervised\n",
    "from mindaffectBCI.decoder.decodingCurveSupervised import decodingCurveSupervised, plot_decoding_curve\n",
    "from mindaffectBCI.decoder.model_fitting import BaseSequence2Sequence, MultiCCA\n",
    "from mindaffectBCI.decoder.scoreOutput import dedupY0\n",
    "from mindaffectBCI.decoder.updateSummaryStatistics import updateSummaryStatistics, plot_summary_statistics, plot_erp\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_to_XY_ndarrays(dataset):\n",
    "    \"\"\"\n",
    "    Convert varitable trial length dataset with stimulus information to fixed wize sklearn-compatible (X,Y) ndarrays.\n",
    "    dataset: a 2-list of (len, dim) arrays. 1st element is the electrode signals, 2nd is the stimulation signals.\n",
    "            Last channel of electrode signals is the timestamps.\n",
    "    \"\"\"\n",
    "    # get length of each trial\n",
    "    tr_lens = [tl[0].shape[0] for tl in dataset]\n",
    "    tr_stim_lens = [tl[1].shape[0] for tl in datset]\n",
    "    \n",
    "    # set array trial length to 90th percentile\n",
    "    tr_len = int(np.percentile(tr_lens, 90))\n",
    "    tr_stim_len = max(20, int(np.percentile(tr_stim_lens, 90)))\n",
    "    # filter the trials to only be the ones long enough to be worth processing\n",
    "    dataset = [d for d in dataset if d[0].shape[0] > tr_len//2 and d[1].shape[0] > tr_stim_len//2]\n",
    "    \n",
    "    # map to single fixed size matrix + upsample stimulus to the EEG/EROS sample rate\n",
    "    Y = np.zeros((len(dataset), tr_len, 256), dtype=dataset[0][1].dtype)\n",
    "    X = np.zeros((len(dataset), tr_len, dataset[0][0].shape[-1]-1), dtype=dataset[0][0].dtype)\n",
    "    X_ts = np.zeros((len(dataset), tr_len), dtype=int)\n",
    "    Y_ts = np.zeros((len(dataset), tr_len), dtype=int)\n",
    "    for ti, (data, stimulus) in enumerate(dataset):\n",
    "        # extract the data, remove the timestamp channel and insert into the ndarray\n",
    "        # guard for slightly different sizes\n",
    "        if X.shape[1] <= data.shape[0]:\n",
    "            X[ti, :, :] = data[:X.shape[1], :-1]\n",
    "            X_ts[ti, :] = data[:X.shape[1], -1]\n",
    "        else: # pad end with final value\n",
    "            X[ti, :data.shape[0], :] = data[:, :-1]\n",
    "            X[ti, data.shape[0]:, :] = data[-1, :-1]\n",
    "            X_ts[ti, :data.shape[0]] = data[:, -1]\n",
    "            \n",
    "        # STIMULUS UPSAMPLING: needs a closer look\n",
    "        data_ts = data[:, -1] # data timestamp per sample\n",
    "        stimulus_ts = stimulus[:, -1] # stimulus timestamp per stimulus ever\n",
    "        stimulus, data_i = upsample_stimseq(data_ts, stimulus[:, :-1], stimulus_ts)\n",
    "        # store -- compensating for any variable trial lengths\n",
    "        if Y.shape[1] < stimulus.shape[0]:  # long trial\n",
    "            Y[ti, :, :] = stimulus[:Y.shape[1], :]\n",
    "        else:  # short trial\n",
    "            Y[ti, :stimulus.shape[0], :] = stimulus\n",
    "        # record stim_ts @ this data_ts\n",
    "        tmp = data_i < Y.shape[1]\n",
    "        Y_ts[ti, data_i[tmp]] = stimulus[tmp]\n",
    "        \n",
    "    return X, Y, X_ts, Y_ts\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(start, end, fs, clsfr: BaseSequence2Sequence, cv=2, previous_dataset=None):\n",
    "    \n",
    "    dataset = getCalibration_dataset(start, end)\n",
    "    if previous_dataset:\n",
    "        dataset.extend(previous_dataset)\n",
    "        \n",
    "    if dataset:\n",
    "        # convert msgs to ndarrays\n",
    "        X, Y, X_ts, Y_ts = dataset_to_XY_ndarrays(dataset)\n",
    "        \n",
    "        try:\n",
    "            pickle.dump(dict(X=X, Y=Y, X_ts=X_ts, Y_ts=Y_ts, fs=fs),\n",
    "                       open('calibration_data.pk', 'wb'))\n",
    "        except:\n",
    "            print('Error saving calibration data')\n",
    "        \n",
    "    if X is None or Y is None:\n",
    "        return None, None\n",
    "    \n",
    "    \n",
    "    # call the classifier fit method\n",
    "    print(f\"Training datset = {X.shape}, {Y.shape}\")\n",
    "    cvscores = clsfr.cv_fit(X Y, cv=cv)\n",
    "    score = np.mean(cvscores['test_score'])\n",
    "    print(f\"classifier={clsfr} => {score}\")\n",
    "    decoding_curve = decodingCurveSupervised(cvscores['estimator'], nInt=(10, 10),\n",
    "                                            priorsigma=(clsfr.sigma0_, clsfr.priorweight),\n",
    "                                            softmaxscale=clsfr.softmaxscale_)\n",
    "    # extract the final estimated performance\n",
    "    print(f\"decoding curve {decoding_curve[1]}\")\n",
    "    print(f\"score {score}\")\n",
    "    perr = decoding_curve[1][-1] if len(decoding_curve) > 1 else 1-score\n",
    "    if CALIBRATIONPLOTS:\n",
    "        try:\n",
    "        #if True:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(1)\n",
    "            clsfr.plot_model(fs=fs)\n",
    "            plt.suptitle('Factored Model')\n",
    "            plt.figure(2)\n",
    "            plot_decoding_curve(*decoding_curve)\n",
    "            plt.suptitle('Decoding Curve')\n",
    "            #  from analyse_datasets import debug_test_dataset\n",
    "            #  debug_test_dataset(X,Y,None,fs=ui.fs)\n",
    "            plt.figure(3) # plot the CCA info\n",
    "            Y_true = clsfr.stim2event(Y)\n",
    "            Y_true = Y_true[...,0:1,:]\n",
    "            Cxx, Cxy, Cyy = updateSummaryStatistics(X,Y_true,tau=clsfr.tau)\n",
    "            plot_summary_statistics(Cxx,Cxy,Cyy,clsfr.evtlabs,fs=fs)\n",
    "            plt.suptitle(\"Summary Statistics\")\n",
    "            try:\n",
    "                pickle.dump(dict(Cxx=Cxx, Cxy=Cxy, Cyy=Cyy, evtlabs=clsfr.evtlabs, fs=fs),\n",
    "                            open('summary_statistics.pk','wb'))\n",
    "            except:\n",
    "                print('Error saving cal data')\n",
    "            plt.figure(4)\n",
    "            plot_erp(Cxy,evtlabs=clsfr.evtlabs,fs=fs)\n",
    "            plt.suptitle(\"Event Related Potential (ERP)\")\n",
    "            plt.show(block=False)\n",
    "            # save figures\n",
    "            logsdir = os.path.join(os.path.dirname(os.path.abspath(__file__)),'../../logs/')\n",
    "            plt.figure(1)\n",
    "            plt.savefig(os.path.join(logsdir,'model_{}.png'.format(uname)))\n",
    "            plt.figure(2)\n",
    "            plt.savefig(os.path.join(logsdir,'decoding_curve_{}.png'.format(uname)))\n",
    "            plt.figure(3)\n",
    "            plt.savefig(os.path.join(logsdir,'summary_statistics_{}.png'.format(uname)))\n",
    "            plt.figure(4)\n",
    "            plt.savefig(os.path.join(logsdir,'erp_{}.png'.format(uname)))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return dataset, X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, X_ts, Y_ts, clsfr: BaseSequence2Sequence, prev_stimulus=None):\n",
    "    \"\"\"\n",
    "    Given the current trials data, apply the classifier and decoder to make target predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    # strip and dedup?\n",
    "    \n",
    "    Fy_1 = clsfr.predict(X, Y, prevY=prev_stimulus)\n",
    "    # map back to 256\n",
    "    Fy = np.zeros(Fy_1.shape[:-1] + (256,), dtype=Fy_1.dtype)\n",
    "    Fy[..., used]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform calibration and prediction on an incrementing train-test split fraction. Initially there is only one trial in the train split and it might correspond to a special calibration stimulation sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  1,  2],\n",
       "        [ 2,  3,  4],\n",
       "        [ 4,  5,  6]],\n",
       "\n",
       "       [[ 8,  9, 10],\n",
       "        [10, 11, 12],\n",
       "        [12, 13, 14]],\n",
       "\n",
       "       [[16, 17, 18],\n",
       "        [18, 19, 20],\n",
       "        [20, 21, 22]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[i + 2*j + 8*k for i in range(3)] for j in range(3)] for k in range(3)])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "oi = np.any(a, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "used = np.any(a.reshape((-1, a.shape[-1])), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
